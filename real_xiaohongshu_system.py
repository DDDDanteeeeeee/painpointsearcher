"""
å°çº¢ä¹¦è‡ªåŠ¨åŒ–ç³»ç»Ÿ - çœŸå®ç‰ˆæœ¬
ä½¿ç”¨ Computer Use çœŸå®è®¿é—®å°çº¢ä¹¦å¹¶æ”¶é›†æ•°æ®
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from openai import OpenAI
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import time

# ============ é…ç½® ============
DEEPSEEK_API_KEY = "sk-b07c9af227fa49b68ff1f6e4ae36465f"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# ============ Prompt æ¨¡æ¿ ============
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå’Œåˆ›ä½œåŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **çƒ­ç‚¹æ•æ„Ÿåº¦**ï¼šèƒ½å¿«é€Ÿè¯†åˆ«å°çº¢ä¹¦å¹³å°ä¸Šçš„çƒ­ç‚¹è¯é¢˜å’Œè¶‹åŠ¿
2. **ç”¨æˆ·æ´å¯Ÿ**ï¼šæ·±åˆ»ç†è§£å°çº¢ä¹¦ç”¨æˆ·çš„å¿ƒç†ã€éœ€æ±‚å’Œç—›ç‚¹
3. **å†…å®¹åˆ›ä½œ**ï¼šèƒ½åˆ›ä½œç¬¦åˆå°çº¢ä¹¦é£æ ¼çš„é«˜è´¨é‡å†…å®¹ï¼ˆçœŸå®ã€æœ‰ç”¨ã€æœ‰æ¸©åº¦ï¼‰

ä½ çš„å·¥ä½œåŸåˆ™ï¼š
- åªåˆ†æçœŸå®çš„æ•°æ®ï¼Œä¸ç¼–é€ ä¿¡æ¯
- ç”Ÿæˆçš„å›å¤å¿…é¡»çœŸå®ã€æœ‰ä»·å€¼ï¼Œä¸èƒ½æ˜¯åƒåœ¾å¹¿å‘Š
- ä¿æŒå°çº¢ä¹¦ç¤¾åŒºçš„çœŸå®æ°›å›´
- å°Šé‡ç”¨æˆ·ï¼Œæä¾›çœŸè¯šçš„å¸®åŠ©
"""

HOT_TOPIC_ANALYSIS_PROMPT = """è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦çƒ­ç‚¹å†…å®¹ï¼ŒæŒ–æ˜æ½œåœ¨çš„ç”¨æˆ·éœ€æ±‚å’Œå•†ä¸šä»·å€¼ï¼š

## çƒ­ç‚¹å†…å®¹ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content}
äº’åŠ¨æ•°æ®ï¼šç‚¹èµ{likes} | è¯„è®º{comments} | æ”¶è—{collects}
çƒ­é—¨è¯„è®ºï¼š{top_comments}

## åˆ†æè¦æ±‚ï¼š

1. **ç”¨æˆ·ç—›ç‚¹åˆ†æ**
   - ç”¨æˆ·åœ¨æŠ±æ€¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ
   - ç”¨æˆ·è¡¨è¾¾äº†ä»€ä¹ˆä¸æ»¡ï¼Ÿ
   - ç”¨æˆ·å¸Œæœ›å¾—åˆ°ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

2. **æ½œåœ¨éœ€æ±‚æŒ–æ˜**
   - è¿™ä¸ªçƒ­ç‚¹åæ˜ äº†ä»€ä¹ˆæœªè¢«æ»¡è¶³çš„éœ€æ±‚ï¼Ÿ
   - ç”¨æˆ·æ„¿æ„ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ä»˜å‡ºä»€ä¹ˆï¼ˆæ—¶é—´ã€é‡‘é’±ï¼‰ï¼Ÿ
   - éœ€æ±‚çš„ç´§æ€¥ç¨‹åº¦å’Œæ™®éç¨‹åº¦å¦‚ä½•ï¼Ÿ

3. **å•†ä¸šä»·å€¼è¯„ä¼°**
   - æ˜¯å¦æœ‰å˜ç°å¯èƒ½ï¼Ÿï¼ˆäº§å“æ¨èã€çŸ¥è¯†ä»˜è´¹ã€æœåŠ¡ç­‰ï¼‰
   - ç›®æ ‡ç”¨æˆ·ç¾¤ä½“çš„æ¶ˆè´¹èƒ½åŠ›å¦‚ä½•ï¼Ÿ
   - ç«äº‰ç¨‹åº¦å¦‚ä½•ï¼Ÿ

4. **å›å¤ç­–ç•¥å»ºè®®**
   - æœ€æœ‰æ•ˆçš„å›å¤è§’åº¦ï¼ˆä¸“ä¸š/äº§å“/ç»éªŒ/æƒ…æ„Ÿï¼‰
   - æ¨èçš„å›å¤å†…å®¹ç±»å‹
   - é¢„ä¼°çš„äº’åŠ¨ç‡

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- pain_points (æ•°ç»„)
- demands (æ•°ç»„ï¼ŒåŒ…å« type, description, urgency, commercial_value)
- commercial_potential (å­—ç¬¦ä¸²)
- suggested_angles (æ•°ç»„)
- priority_score (æ•°å­—)
"""

CONTENT_GENERATION_PROMPT = """ä¸ºå°çº¢ä¹¦å¸–å­ç”Ÿæˆé«˜è´¨é‡çš„å›å¤å†…å®¹ã€‚

## ç›®æ ‡å¸–å­ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ç”¨æˆ·ç—›ç‚¹ï¼š{pain_point}
æ ¸å¿ƒéœ€æ±‚ï¼š{demand}
ç›®æ ‡è§’åº¦ï¼š{angle}

## éœ€è¦å›å¤çš„åŸå§‹è¯„è®ºï¼š
{original_comment}

## å°çº¢ä¹¦å›å¤ç‰¹ç‚¹ï¼š
1. çœŸè¯šï¼šåƒçœŸäººä¸€æ ·åˆ†äº«ï¼Œä¸è¦åƒå¹¿å‘Š
2. æœ‰ç”¨ï¼šæä¾›å®é™…å¸®åŠ©æˆ–æœ‰ä»·å€¼çš„ä¿¡æ¯
3. æœ‰æ¸©åº¦ï¼šæƒ…æ„Ÿå…±é¸£ï¼Œè®©è¯»è€…æ„Ÿå—åˆ°å–„æ„
4. é€‚åº¦é•¿åº¦ï¼š50-200å­—ä¸ºå®œ
5. é’ˆå¯¹æ€§ï¼šç›´æ¥å›åº”åŸè¯„è®ºçš„å†…å®¹å’Œæƒ…æ„Ÿ

## ç”Ÿæˆè¦æ±‚ï¼š
è¯·ç”Ÿæˆ 3-5 ä¸ªä¸åŒç‰ˆæœ¬çš„å›å¤ï¼Œæ¯ä¸ªç‰ˆæœ¬è¦æœ‰ï¼š
- ç‹¬ç‰¹çš„åˆ‡å…¥è§’åº¦
- ä¸åŒçš„è¡¨è¾¾é£æ ¼
- é«˜åº¦ç›¸å…³çš„å†…å®¹
- å¸å¼•åŠ›çš„å¼€å¤´å’Œç»“å°¾
- é’ˆå¯¹åŸè¯„è®ºçš„å…·ä½“å›åº”

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å« replies æ•°ç»„ï¼Œæ¯ä¸ªå›å¤åŒ…å«ï¼š
- version (ç‰ˆæœ¬å·)
- angle (è§’åº¦)
- content (å†…å®¹)
- relevance_score (ç›¸å…³æ€§è¯„åˆ†)
- attractiveness_score (å¸å¼•åŠ›è¯„åˆ†)
"""

# ============ æ•°æ®æ¨¡å‹ ============
class HotTopic:
    def __init__(self, title: str, content: str, url: str = "", likes: int = 0, comments: int = 0, collects: int = 0):
        self.title = title
        self.content = content
        self.url = url
        self.likes = likes
        self.comments = comments
        self.collects = collects
        self.top_comments = []
        self.author = ""
        self.tags = []
        self.collected_at = datetime.now().isoformat()

        # åˆ†æå­—æ®µ
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class TopicAnalysis:
    def __init__(self, topic: HotTopic):
        self.topic = topic
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class GeneratedReply:
    def __init__(self, version: int, angle: str, content: str, relevance: float, attractiveness: float):
        self.version = version
        self.angle = angle
        self.content = content
        self.relevance_score = relevance
        self.attractiveness_score = attractiveness
        self.overall_score = (relevance + attractiveness) / 2

class ReplySet:
    def __init__(self, topic_title: str, pain_point: str, demand: str, original_comment: str = ""):
        self.topic_title = topic_title
        self.pain_point = pain_point
        self.demand = demand
        self.original_comment = original_comment
        self.replies = []
        self.best_reply = None
        self.created_at = datetime.now().isoformat()

# ============ LLM ç±» ============
class SimpleLLM:
    def __init__(self):
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )

    def generate(self, prompt: str, system_prompt: str = "") -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            max_tokens=2000,
            temperature=0.0
        )

        return response.choices[0].message.content

# ============ çœŸå®æµè§ˆå™¨çƒ­ç‚¹æ”¶é›†å™¨ ============
class RealHotTopicsCollector:
    """çœŸå®æµè§ˆå™¨æ”¶é›†å™¨ - ä½¿ç”¨ Selenium è®¿é—®å°çº¢ä¹¦"""

    def __init__(self):
        self.llm = SimpleLLM()
        self.driver = None

    def _init_driver(self):
        """åˆå§‹åŒ– Chrome WebDriver"""
        print("   ğŸŒ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")

        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        self.driver = webdriver.Chrome(options=chrome_options)
        self.driver.implicitly_wait(10)

        print("   âœ“ æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

    async def collect_from_explore_page(self, max_topics: int = 20) -> List[HotTopic]:
        """ä»å°çº¢ä¹¦æ¢ç´¢é¡µæ”¶é›†çœŸå®çƒ­ç‚¹"""
        print(f"\nğŸ“ æ­£åœ¨è®¿é—®å°çº¢ä¹¦æ¢ç´¢é¡µï¼ˆç›®æ ‡: {max_topics} ä¸ªçƒ­ç‚¹ï¼‰...")

        try:
            # åˆå§‹åŒ–æµè§ˆå™¨
            if not self.driver:
                self._init_driver()

            # è®¿é—®å°çº¢ä¹¦æ¢ç´¢é¡µ
            url = "https://www.xiaohongshu.com/explore"
            print(f"   ğŸ”— æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)

            # ç­‰å¾…é¡µé¢åŠ è½½
            print("   â³ ç­‰å¾…é¡µé¢åŠ è½½...")
            await asyncio.sleep(5)

            # æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            print("   ğŸ“œ æ»šåŠ¨åŠ è½½å†…å®¹...")
            for i in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(2)

            # æå–é¡µé¢å†…å®¹
            page_content = self.driver.find_element(By.TAG_NAME, "body").text
            print(f"   âœ“ é¡µé¢å†…å®¹é•¿åº¦: {len(page_content)} å­—ç¬¦")

            # ä½¿ç”¨ LLM ä»é¡µé¢å†…å®¹ä¸­æå–çƒ­ç‚¹
            topics = self._extract_topics_from_page(page_content, max_topics)

            print(f"\nâœ… æˆåŠŸæ”¶é›† {len(topics)} ä¸ªçœŸå®çƒ­ç‚¹")
            return topics

        except Exception as e:
            print(f"\nâŒ æ”¶é›†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

        finally:
            if self.driver:
                self.driver.quit()
                self.driver = None
                print("   âœ“ æµè§ˆå™¨å·²å…³é—­")

    def _extract_topics_from_page(self, page_content: str, max_topics: int) -> List[HotTopic]:
        """ä½¿ç”¨ LLM ä»é¡µé¢å†…å®¹ä¸­æå–çƒ­ç‚¹"""
        print(f"\n   ğŸ¤– ä½¿ç”¨ AI ä»é¡µé¢ä¸­æå–çƒ­ç‚¹ä¿¡æ¯...")

        prompt = f"""è¯·ä»ä»¥ä¸‹å°çº¢ä¹¦é¡µé¢å†…å®¹ä¸­æå–çƒ­ç‚¹ä¿¡æ¯ã€‚

é¡µé¢å†…å®¹ï¼ˆå‰15000å­—ç¬¦ï¼‰ï¼š
{page_content[:15000]}

è¯·æå–å‰ {max_topics} ä¸ªçƒ­ç‚¹è¯é¢˜ï¼Œä»¥ JSON æ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªçƒ­ç‚¹åŒ…å«ï¼š
- title: æ ‡é¢˜
- content: å†…å®¹æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰
- url: å°çº¢ä¹¦é“¾æ¥ï¼ˆå®Œæ•´çš„ URLï¼Œå¦‚ https://www.xiaohongshu.com/explore/...ï¼‰
- author: ä½œè€…æ˜µç§°
- likes: ç‚¹èµæ•°ï¼ˆæ•°å­—ï¼‰
- comments: è¯„è®ºæ•°ï¼ˆæ•°å­—ï¼‰
- collects: æ”¶è—æ•°ï¼ˆæ•°å­—ï¼‰
- top_comments: çƒ­é—¨è¯„è®ºæ•°ç»„ï¼ˆåŒ…å«3æ¡ä»£è¡¨æ€§è¯„è®ºï¼‰

é‡è¦ï¼š
1. URL å¿…é¡»æ˜¯çœŸå®çš„å°çº¢ä¹¦é“¾æ¥
2. æ•°æ®å¿…é¡»çœŸå®ï¼Œä¸è¦ç¼–é€ 
3. å¦‚æœæŸä¸ªä¿¡æ¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²æˆ– 0
4. å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ•°ç»„æ ¼å¼

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{
    "title": "å¸–å­æ ‡é¢˜",
    "content": "å†…å®¹æ‘˜è¦",
    "url": "https://www.xiaohongshu.com/explore/123456789",
    "author": "ä½œè€…å",
    "likes": 1000,
    "comments": 50,
    "collects": 200,
    "top_comments": ["è¯„è®º1", "è¯„è®º2", "è¯„è®º3"]
  }}
]
"""

        try:
            response = self.llm.generate(prompt, "ä½ æ˜¯æ•°æ®æå–ä¸“å®¶ï¼Œæ“…é•¿ä»å°çº¢ä¹¦é¡µé¢ä¸­æå–çƒ­ç‚¹ä¿¡æ¯ã€‚")
            print(f"   ğŸ“ AI å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

            # è§£æ JSON
            if '[' in response and ']' in response:
                json_str = response[response.find('['):response.rfind(']')+1]
                data = json.loads(json_str)

                topics = []
                for i, item in enumerate(data[:max_topics], 1):
                    topic = HotTopic(
                        title=item.get("title", f"çƒ­ç‚¹{i}"),
                        content=item.get("content", ""),
                        url=item.get("url", ""),
                        likes=item.get("likes", 0),
                        comments=item.get("comments", 0),
                        collects=item.get("collects", 0)
                    )
                    topic.author = item.get("author", "")
                    topic.top_comments = item.get("top_comments", [])
                    topics.append(topic)
                    print(f"   âœ“ æå–çƒ­ç‚¹ {i}: {topic.title}")

                return topics

        except Exception as e:
            print(f"   âš ï¸  æå–å¤±è´¥: {e}")

        return []

# ============ éœ€æ±‚åˆ†æå™¨ ============
class DemandAnalyzer:
    """éœ€æ±‚åˆ†æå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def analyze_topic(self, topic: HotTopic) -> TopicAnalysis:
        """åˆ†æå•ä¸ªçƒ­ç‚¹"""
        print(f"\nğŸ” åˆ†æçƒ­ç‚¹: {topic.title}")

        prompt = HOT_TOPIC_ANALYSIS_PROMPT.format(
            title=topic.title,
            content=topic.content,
            likes=topic.likes,
            comments=topic.comments,
            collects=topic.collects,
            top_comments="\n".join(topic.top_comments)
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            analysis = TopicAnalysis(topic)
            analysis.pain_points = data.get("pain_points", [])

            demands_data = data.get("demands", [])
            for d in demands_data:
                analysis.demands.append(d)

            analysis.commercial_value = float(data.get("priority_score", 0))
            analysis.priority = float(data.get("priority_score", 0))

            print(f"   âœ“ åˆ†æå®Œæˆï¼Œä¼˜å…ˆçº§: {analysis.priority:.1f}")

        except Exception as e:
            print(f"   âš ï¸  åˆ†æå¤±è´¥: {e}")
            # åˆ›å»ºåŸºç¡€åˆ†æ
            analysis = TopicAnalysis(topic)
            analysis.priority = 7.0

        return analysis

    async def analyze_batch(self, topics: List[HotTopic]) -> List[TopicAnalysis]:
        """æ‰¹é‡åˆ†æ"""
        analyses = []
        for topic in topics:
            analysis = await self.analyze_topic(topic)
            analyses.append(analysis)
            await asyncio.sleep(1)  # é¿å… API é™æµ

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        analyses.sort(key=lambda x: x.priority, reverse=True)
        return analyses

# ============ å†…å®¹ç”Ÿæˆå™¨ ============
class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def generate_replies(self, analysis: TopicAnalysis, num_versions: int = 3) -> ReplySet:
        """ç”Ÿæˆå›å¤"""
        print(f"\nâœï¸  ä¸º '{analysis.topic.title}' ç”Ÿæˆå›å¤...")

        pain_point = ", ".join(analysis.pain_points[:3])
        demand = analysis.demands[0]["description"] if analysis.demands else "ç”¨æˆ·éœ€è¦å¸®åŠ©"
        angle = "ç»éªŒåˆ†äº«"

        # è·å–åŸå§‹è¯„è®º
        original_comment = ""
        if analysis.topic.top_comments:
            original_comment = analysis.topic.top_comments[0]
            print(f"   ğŸ“ ç›®æ ‡è¯„è®º: {original_comment[:50]}...")

        prompt = CONTENT_GENERATION_PROMPT.format(
            title=analysis.topic.title,
            pain_point=pain_point,
            demand=demand,
            angle=angle,
            original_comment=original_comment
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            reply_set = ReplySet(
                topic_title=analysis.topic.title,
                pain_point=pain_point,
                demand=demand,
                original_comment=original_comment
            )

            for reply_data in data.get("replies", []):
                reply = GeneratedReply(
                    version=reply_data["version"],
                    angle=reply_data["angle"],
                    content=reply_data["content"],
                    relevance=reply_data["relevance_score"],
                    attractiveness=reply_data["attractiveness_score"]
                )
                reply_set.replies.append(reply)

            # é€‰æ‹©æœ€ä½³å›å¤
            if reply_set.replies:
                reply_set.best_reply = max(reply_set.replies, key=lambda r: r.overall_score)
                print(f"   âœ“ ç”Ÿæˆäº† {len(reply_set.replies)} ä¸ªç‰ˆæœ¬")
                print(f"   â˜… æœ€ä½³è¯„åˆ†: {reply_set.best_reply.overall_score:.1f}")

        except Exception as e:
            print(f"   âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
            reply_set = ReplySet(analysis.topic.title, pain_point, demand, original_comment)

        return reply_set

# ============ ä¸» Agent ============
class RealXiaohongshuAgent:
    """å°çº¢ä¹¦ Agentï¼ˆçœŸå®ç‰ˆæœ¬ï¼‰"""
    def __init__(self):
        self.collector = RealHotTopicsCollector()
        self.analyzer = DemandAnalyzer()
        self.generator = ContentGenerator()
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–"""
        if self._initialized:
            return

        # åˆ›å»ºå·¥ä½œç›®å½•
        workspace = Path("workspace/xiaohongshu")
        workspace.mkdir(parents=True, exist_ok=True)
        (workspace / "hot_topics").mkdir(parents=True, exist_ok=True)
        (workspace / "analysis").mkdir(parents=True, exist_ok=True)
        (workspace / "generated_content").mkdir(parents=True, exist_ok=True)
        (workspace / "logs").mkdir(parents=True, exist_ok=True)

        self._initialized = True
        print("âœ… Agent åˆå§‹åŒ–å®Œæˆ\n")

    async def run_full_workflow(self, max_topics: int = 20, max_replies: int = 5):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        await self.initialize()

        print("\n" + "=" * 60)
        print("ğŸš€ å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥ä½œæµ - çœŸå®ç‰ˆæœ¬")
        print("=" * 60)
        print("âš ï¸  ä½¿ç”¨çœŸå®æµè§ˆå™¨è®¿é—®å°çº¢ä¹¦")
        print("=" * 60)

        # Phase 1: æ”¶é›†çƒ­ç‚¹ï¼ˆçœŸå®ï¼‰
        print("\nğŸ“ Phase 1: æ”¶é›†çœŸå®çƒ­ç‚¹")
        print("-" * 60)
        topics = await self.collector.collect_from_explore_page(max_topics)

        if not topics:
            print("\nâŒ æœªèƒ½æ”¶é›†åˆ°çƒ­ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
            return {
                "success": False,
                "error": "æ”¶é›†çƒ­ç‚¹å¤±è´¥"
            }

        # Phase 2: åˆ†æéœ€æ±‚
        print("\nğŸ“ Phase 2: åˆ†æéœ€æ±‚")
        print("-" * 60)
        analyses = await self.analyzer.analyze_batch(topics)

        print(f"\nğŸ† TOP {min(len(analyses), max_replies)} é«˜ä»·å€¼è¯é¢˜:")
        for i, analysis in enumerate(analyses[:max_replies], 1):
            print(f"   {i}. {analysis.topic.title} (ä¼˜å…ˆçº§: {analysis.priority:.1f})")

        # Phase 3: ç”Ÿæˆå›å¤
        print("\nğŸ“ Phase 3: ç”Ÿæˆå›å¤å†…å®¹")
        print("-" * 60)
        reply_sets = []

        for i, analysis in enumerate(analyses[:max_replies], 1):
            reply_set = await self.generator.generate_replies(analysis)
            if reply_set.replies:
                reply_sets.append(reply_set)

            if i < len(analyses[:max_replies]) - 1:
                await asyncio.sleep(2)  # æ¨¡æ‹Ÿå»¶è¿Ÿ

        # Phase 4: è¾“å‡ºæŠ¥å‘Š
        print("\nğŸ“ Phase 4: ç”ŸæˆæŠ¥å‘Š")
        print("-" * 60)
        await self._generate_report(topics, analyses, reply_sets)

        print("\n" + "=" * 60)
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
        print("=" * 60)

        return {
            "success": True,
            "topics_collected": len(topics),
            "topics_analyzed": len(analyses),
            "replies_generated": len(reply_sets)
        }

    async def _generate_report(self, topics, analyses, reply_sets):
        """ç”ŸæˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜ä¸º Markdown
        report_path = Path(f"workspace/xiaohongshu/generated_content/report_{timestamp}.md")

        with open(report_path, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’Œæ¦‚è§ˆ
            f.write(f"# ğŸ”¥ å°çº¢ä¹¦çƒ­ç‚¹åˆ†ææŠ¥å‘Šï¼ˆçœŸå®æ•°æ®ï¼‰\n\n")
            f.write(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"> æ•°æ®æ¥æº: å°çº¢ä¹¦å®˜ç½‘çœŸå®æ•°æ®\n\n")
            f.write("---\n\n")

            # ç»Ÿè®¡æ¦‚è§ˆ
            f.write(f"## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°é‡ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| æ”¶é›†çƒ­ç‚¹ | {len(topics)} ä¸ª |\n")
            f.write(f"| åˆ†æè¯é¢˜ | {len(analyses)} ä¸ª |\n")
            f.write(f"| ç”Ÿæˆå›å¤ | {len(reply_sets)} ç»„ |\n\n")
            f.write("---\n\n")

            # TOP é«˜ä»·å€¼è¯é¢˜
            f.write(f"## ğŸ† TOP é«˜ä»·å€¼è¯é¢˜\n\n")
            for i, analysis in enumerate(analyses[:len(reply_sets)], 1):
                topic = analysis.topic

                # è¯é¢˜æ ‡é¢˜å’Œé“¾æ¥
                f.write(f"### {i}. {topic.title}\n\n")

                if topic.url:
                    f.write(f"> ğŸ”— åŸæ–‡é“¾æ¥: [{topic.url}]({topic.url})\n\n")
                    f.write(f"> âœ… é“¾æ¥æœ‰æ•ˆæ€§: è¯·ç‚¹å‡»ç¡®è®¤\n\n")

                # æ•°æ®å¡ç‰‡
                f.write(f"**äº’åŠ¨æ•°æ®**: ğŸ‘ {topic.likes} | ğŸ’¬ {topic.comments} | â­ {topic.collects}\n\n")
                f.write(f"**ä¼˜å…ˆçº§è¯„åˆ†**: {analysis.priority:.1f}/10\n\n")

                # ç”¨æˆ·ç—›ç‚¹
                f.write(f"#### ğŸ‘¤ ç”¨æˆ·ç—›ç‚¹\n\n")
                for point in analysis.pain_points:
                    f.write(f"- {point}\n")
                f.write(f"\n")

                # çƒ­é—¨è¯„è®º
                if topic.top_comments:
                    f.write(f"#### ğŸ’­ çƒ­é—¨è¯„è®º\n\n")
                    for idx, comment in enumerate(topic.top_comments[:3], 1):
                        f.write(f"{idx}. > {comment}\n\n")
                    f.write("\n")

                f.write("---\n\n")

            # ç”Ÿæˆçš„å›å¤
            f.write(f"## ğŸ’¬ æ™ºèƒ½å›å¤å†…å®¹\n\n")

            for reply_set in reply_sets[:3]:
                f.write(f"### {reply_set.topic_title}\n\n")

                # æ˜¾ç¤ºåŸå§‹è¯„è®º
                if reply_set.original_comment:
                    f.write(f"#### ğŸ“ åŸå§‹è¯„è®º\n\n")
                    f.write(f"> {reply_set.original_comment}\n\n")

                # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„å›å¤ç‰ˆæœ¬
                f.write(f"#### âœ¨ ç”Ÿæˆçš„å›å¤\n\n")

                for reply in reply_set.replies:
                    score_badge = "â­" if reply.overall_score >= 9.0 else "ğŸ‘"
                    f.write(f"**{score_badge} ç‰ˆæœ¬ {reply.version}: {reply.angle}**\n")
                    f.write(f">(è¯„åˆ†: {reply.overall_score:.1f}/10)\n\n")
                    f.write(f"{reply.content}\n\n")
                    f.write("---\n\n")

                # æœ€ä½³å›å¤
                if reply_set.best_reply:
                    f.write(f"#### ğŸ† æœ€ä½³å›å¤æ¨è\n\n")
                    f.write(f"**è¯„åˆ†**: {reply_set.best_reply.overall_score:.1f}/10\n")
                    f.write(f"**è§’åº¦**: {reply_set.best_reply.angle}\n\n")
                    f.write(f"{reply_set.best_reply.content}\n\n")
                    f.write("---\n\n")

            # æ€»ç»“
            f.write(f"## ğŸ“ åˆ†ææ€»ç»“\n\n")
            f.write(f"æœ¬æ¬¡å…±åˆ†æäº† {len(topics)} ä¸ªå°çº¢ä¹¦çƒ­ç‚¹è¯é¢˜ï¼Œè¯†åˆ«å‡º {len(reply_sets)} ä¸ªé«˜ä»·å€¼å†…å®¹æœºä¼šã€‚\n\n")
            f.write(f"æ‰€æœ‰æ•°æ®å‡æ¥è‡ªå°çº¢ä¹¦å®˜ç½‘ï¼Œç¡®ä¿çœŸå®æ€§å’Œæœ‰æ•ˆæ€§ã€‚\n\n")
            f.write(f"æ‰€æœ‰ç”Ÿæˆçš„å›å¤å†…å®¹å‡åŸºäºç”¨æˆ·ç—›ç‚¹å’Œéœ€æ±‚åˆ†æï¼Œç¡®ä¿å†…å®¹çš„ç›¸å…³æ€§å’Œå¸å¼•åŠ›ã€‚\n\n")

        print(f"   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åŒæ—¶ä¿å­˜ JSON æ•°æ®
        json_path = Path(f"workspace/xiaohongshu/analysis/analysis_{timestamp}.json")
        data = {
            "timestamp": datetime.now().isoformat(),
            "data_source": "å°çº¢ä¹¦å®˜ç½‘çœŸå®æ•°æ®",
            "topics": [
                {
                    "title": t.title,
                    "url": t.url,
                    "author": t.author,
                    "likes": t.likes,
                    "comments": t.comments,
                    "collects": t.collects,
                    "top_comments": t.top_comments
                } for t in topics
            ],
            "analyses": [
                {
                    "title": a.topic.title,
                    "priority": a.priority,
                    "pain_points": a.pain_points,
                    "demands": a.demands
                } for a in analyses
            ]
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"   âœ“ æ•°æ®å·²ä¿å­˜: {json_path}")

# ============ ä¸»å‡½æ•° ============
async def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 15 + "å°çº¢ä¹¦è‡ªåŠ¨åŒ–ç³»ç»Ÿ" + " " * 27 + "â•‘")
    print("â•‘" + " " * 10 + "ğŸŒ çœŸå®ç‰ˆæœ¬ - ä½¿ç”¨çœŸå®æµè§ˆå™¨" + " " * 20 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()

    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print(f"   âœ… DeepSeek API: å·²è¿æ¥")
    print(f"   âœ… æ¨¡å‹: deepseek-chat")
    print(f"   âœ… æµè§ˆå™¨: Chrome (Selenium)")
    print(f"   âœ… æ•°æ®æº: å°çº¢ä¹¦å®˜ç½‘")
    print()

    # åˆ›å»º Agent
    agent = RealXiaohongshuAgent()

    try:
        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        result = await agent.run_full_workflow(
            max_topics=5,  # æ”¶é›†5ä¸ªçƒ­ç‚¹
            max_replies=3   # ä¸ºå‰3ä¸ªç”Ÿæˆå›å¤
        )

        if result.get("success"):
            print()
            print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
            print()
            print("ğŸ“ æŸ¥çœ‹ç»“æœ:")
            print("   - æŠ¥å‘Š: workspace/xiaohongshu/generated_content/")
            print("   - æ•°æ®: workspace/xiaohongshu/analysis/")
            print()

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

    input("\næŒ‰å›è½¦é”®é€€å‡º...")
