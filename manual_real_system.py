"""
å°çº¢ä¹¦è‡ªåŠ¨åŒ–ç³»ç»Ÿ - çœŸå®ç‰ˆæœ¬ï¼ˆæ‰‹åŠ¨è¾“å…¥URLï¼‰
ç”¨æˆ·è¾“å…¥çœŸå®çš„å°çº¢ä¹¦é“¾æ¥ï¼Œç³»ç»ŸæŠ“å–å¹¶åˆ†æ
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from openai import OpenAI
import requests
from bs4 import BeautifulSoup
import time

# ============ é…ç½® ============
DEEPSEEK_API_KEY = "sk-b07c9af227fa49b68ff1f6e4ae36465f"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# ============ Prompt æ¨¡æ¿ ============
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå’Œåˆ›ä½œåŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **çƒ­ç‚¹æ•æ„Ÿåº¦**ï¼šèƒ½å¿«é€Ÿè¯†åˆ«å°çº¢ä¹¦å¹³å°ä¸Šçš„çƒ­ç‚¹è¯é¢˜å’Œè¶‹åŠ¿
2. **ç”¨æˆ·æ´å¯Ÿ**ï¼šæ·±åˆ»ç†è§£å°çº¢ä¹¦ç”¨æˆ·çš„å¿ƒç†ã€éœ€æ±‚å’Œç—›ç‚¹
3. **å†…å®¹åˆ›ä½œ**ï¼šèƒ½åˆ›ä½œç¬¦åˆå°çº¢ä¹¦é£æ ¼çš„é«˜è´¨é‡å†…å®¹ï¼ˆçœŸå®ã€æœ‰ç”¨ã€æœ‰æ¸©åº¦ï¼‰

ä½ çš„å·¥ä½œåŸåˆ™ï¼š
- åªåˆ†æçœŸå®çš„æ•°æ®ï¼Œä¸ç¼–é€ ä¿¡æ¯
- ç”Ÿæˆçš„å›å¤å¿…é¡»çœŸå®ã€æœ‰ä»·å€¼ï¼Œä¸èƒ½æ˜¯åƒåœ¾å¹¿å‘Š
- ä¿æŒå°çº¢ä¹¦ç¤¾åŒºçš„çœŸå®æ°›å›´
- å°Šé‡ç”¨æˆ·ï¼Œæä¾›çœŸè¯šçš„å¸®åŠ©
"""

HOT_TOPIC_ANALYSIS_PROMPT = """è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦çƒ­ç‚¹å†…å®¹ï¼ŒæŒ–æ˜æ½œåœ¨çš„ç”¨æˆ·éœ€æ±‚å’Œå•†ä¸šä»·å€¼ï¼š

## çƒ­ç‚¹å†…å®¹ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content}
äº’åŠ¨æ•°æ®ï¼šç‚¹èµ{likes} | è¯„è®º{comments} | æ”¶è—{collects}
çƒ­é—¨è¯„è®ºï¼š{top_comments}

## åˆ†æè¦æ±‚ï¼š

1. **ç”¨æˆ·ç—›ç‚¹åˆ†æ**
   - ç”¨æˆ·åœ¨æŠ±æ€¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ
   - ç”¨æˆ·è¡¨è¾¾äº†ä»€ä¹ˆä¸æ»¡ï¼Ÿ
   - ç”¨æˆ·å¸Œæœ›å¾—åˆ°ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

2. **æ½œåœ¨éœ€æ±‚æŒ–æ˜**
   - è¿™ä¸ªçƒ­ç‚¹åæ˜ äº†ä»€ä¹ˆæœªè¢«æ»¡è¶³çš„éœ€æ±‚ï¼Ÿ
   - ç”¨æˆ·æ„¿æ„ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ä»˜å‡ºä»€ä¹ˆï¼ˆæ—¶é—´ã€é‡‘é’±ï¼‰ï¼Ÿ
   - éœ€æ±‚çš„ç´§æ€¥ç¨‹åº¦å’Œæ™®éç¨‹åº¦å¦‚ä½•ï¼Ÿ

3. **å•†ä¸šä»·å€¼è¯„ä¼°**
   - æ˜¯å¦æœ‰å˜ç°å¯èƒ½ï¼Ÿï¼ˆäº§å“æ¨èã€çŸ¥è¯†ä»˜è´¹ã€æœåŠ¡ç­‰ï¼‰
   - ç›®æ ‡ç”¨æˆ·ç¾¤ä½“çš„æ¶ˆè´¹èƒ½åŠ›å¦‚ä½•ï¼Ÿ
   - ç«äº‰ç¨‹åº¦å¦‚ä½•ï¼Ÿ

4. **å›å¤ç­–ç•¥å»ºè®®**
   - æœ€æœ‰æ•ˆçš„å›å¤è§’åº¦ï¼ˆä¸“ä¸š/äº§å“/ç»éªŒ/æƒ…æ„Ÿï¼‰
   - æ¨èçš„å›å¤å†…å®¹ç±»å‹
   - é¢„ä¼°çš„äº’åŠ¨ç‡

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- pain_points (æ•°ç»„)
- demands (æ•°ç»„ï¼ŒåŒ…å« type, description, urgency, commercial_value)
- commercial_potential (å­—ç¬¦ä¸²)
- suggested_angles (æ•°ç»„)
- priority_score (æ•°å­—)
"""

CONTENT_GENERATION_PROMPT = """ä¸ºå°çº¢ä¹¦å¸–å­ç”Ÿæˆé«˜è´¨é‡çš„å›å¤å†…å®¹ã€‚

## ç›®æ ‡å¸–å­ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ç”¨æˆ·ç—›ç‚¹ï¼š{pain_point}
æ ¸å¿ƒéœ€æ±‚ï¼š{demand}
ç›®æ ‡è§’åº¦ï¼š{angle}

## éœ€è¦å›å¤çš„åŸå§‹è¯„è®ºï¼š
{original_comment}

## å°çº¢ä¹¦å›å¤ç‰¹ç‚¹ï¼š
1. çœŸè¯šï¼šåƒçœŸäººä¸€æ ·åˆ†äº«ï¼Œä¸è¦åƒå¹¿å‘Š
2. æœ‰ç”¨ï¼šæä¾›å®é™…å¸®åŠ©æˆ–æœ‰ä»·å€¼çš„ä¿¡æ¯
3. æœ‰æ¸©åº¦ï¼šæƒ…æ„Ÿå…±é¸£ï¼Œè®©è¯»è€…æ„Ÿå—åˆ°å–„æ„
4. é€‚åº¦é•¿åº¦ï¼š50-200å­—ä¸ºå®œ
5. é’ˆå¯¹æ€§ï¼šç›´æ¥å›åº”åŸè¯„è®ºçš„å†…å®¹å’Œæƒ…æ„Ÿ

## ç”Ÿæˆè¦æ±‚ï¼š
è¯·ç”Ÿæˆ 3-5 ä¸ªä¸åŒç‰ˆæœ¬çš„å›å¤ï¼Œæ¯ä¸ªç‰ˆæœ¬è¦æœ‰ï¼š
- ç‹¬ç‰¹çš„åˆ‡å…¥è§’åº¦
- ä¸åŒçš„è¡¨è¾¾é£æ ¼
- é«˜åº¦ç›¸å…³çš„å†…å®¹
- å¸å¼•åŠ›çš„å¼€å¤´å’Œç»“å°¾
- é’ˆå¯¹åŸè¯„è®ºçš„å…·ä½“å›åº”

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å« replies æ•°ç»„ï¼Œæ¯ä¸ªå›å¤åŒ…å«ï¼š
- version (ç‰ˆæœ¬å·)
- angle (è§’åº¦)
- content (å†…å®¹)
- relevance_score (ç›¸å…³æ€§è¯„åˆ†)
- attractiveness_score (å¸å¼•åŠ›è¯„åˆ†)
"""

# ============ æ•°æ®æ¨¡å‹ ============
class HotTopic:
    def __init__(self, title: str, content: str, url: str = ""):
        self.title = title
        self.content = content
        self.url = url
        self.likes = 0
        self.comments = 0
        self.collects = 0
        self.top_comments = []
        self.author = ""
        self.tags = []
        self.collected_at = datetime.now().isoformat()

        # åˆ†æå­—æ®µ
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class TopicAnalysis:
    def __init__(self, topic: HotTopic):
        self.topic = topic
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class GeneratedReply:
    def __init__(self, version: int, angle: str, content: str, relevance: float, attractiveness: float):
        self.version = version
        self.angle = angle
        self.content = content
        self.relevance_score = relevance
        self.attractiveness_score = attractiveness
        self.overall_score = (relevance + attractiveness) / 2

class ReplySet:
    def __init__(self, topic_title: str, pain_point: str, demand: str, original_comment: str = ""):
        self.topic_title = topic_title
        self.pain_point = pain_point
        self.demand = demand
        self.original_comment = original_comment
        self.replies = []
        self.best_reply = None
        self.created_at = datetime.now().isoformat()

# ============ LLM ç±» ============
class SimpleLLM:
    def __init__(self):
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )

    def generate(self, prompt: str, system_prompt: str = "") -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            max_tokens=2000,
            temperature=0.0
        )

        return response.choices[0].message.content

# ============ çœŸå®æ•°æ®æ”¶é›†å™¨ï¼ˆæ‰‹åŠ¨è¾“å…¥URLï¼‰ ============
class ManualURLCollector:
    """æ‰‹åŠ¨è¾“å…¥URLæ”¶é›†å™¨"""

    def __init__(self):
        self.llm = SimpleLLM()

    async def collect_from_url(self, url: str) -> Optional[HotTopic]:
        """ä»æŒ‡å®šçš„URLæ”¶é›†æ•°æ®"""
        print(f"\nğŸ“ æ­£åœ¨è®¿é—®: {url}")

        try:
            # ä½¿ç”¨ requests è·å–é¡µé¢
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }

            print("   ğŸ“¡ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            print(f"   âœ“ é¡µé¢è·å–æˆåŠŸï¼ˆ{len(response.text)} å­—ç¬¦ï¼‰")

            # ä½¿ç”¨ BeautifulSoup è§£æ
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–é¡µé¢æ ‡é¢˜
            page_title = soup.find('title')
            title = page_title.text.strip() if page_title else "æœªçŸ¥æ ‡é¢˜"

            # æå–é¡µé¢æ–‡æœ¬å†…å®¹
            page_text = soup.get_text(separator='\n', strip=True)

            # é™åˆ¶é•¿åº¦
            content = page_text[:3000] if len(page_text) > 3000 else page_text

            # ä½¿ç”¨ LLM æå–ç»“æ„åŒ–æ•°æ®
            print("   ğŸ¤– ä½¿ç”¨ AI åˆ†æé¡µé¢å†…å®¹...")
            topic = self._extract_with_llm(title, content, url)

            if topic:
                print(f"   âœ“ æˆåŠŸæå–: {topic.title}")

            return topic

        except Exception as e:
            print(f"   âŒ è·å–å¤±è´¥: {e}")
            return None

    def _extract_with_llm(self, title: str, content: str, url: str) -> Optional[HotTopic]:
        """ä½¿ç”¨ LLM ä»é¡µé¢å†…å®¹ä¸­æå–ç»“æ„åŒ–æ•°æ®"""
        prompt = f"""è¯·ä»ä»¥ä¸‹å°çº¢ä¹¦é¡µé¢å†…å®¹ä¸­æå–çƒ­ç‚¹ä¿¡æ¯ã€‚

é¡µé¢æ ‡é¢˜ï¼š{title}

é¡µé¢å†…å®¹ï¼š
{content}

URL: {url}

è¯·æå–å¹¶è¿”å› JSON æ ¼å¼çš„çƒ­ç‚¹ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
- title: å¸–å­æ ‡é¢˜
- content: å†…å®¹æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰
- author: ä½œè€…æ˜µç§°
- likes: ç‚¹èµæ•°ï¼ˆæ•°å­—ï¼‰
- comments: è¯„è®ºæ•°ï¼ˆæ•°å­—ï¼‰
- collects: æ”¶è—æ•°ï¼ˆæ•°å­—ï¼‰
- top_comments: çƒ­é—¨è¯„è®ºæ•°ç»„ï¼ˆåŒ…å«3æ¡ä»£è¡¨æ€§è¯„è®ºï¼‰

æ³¨æ„ï¼š
1. å¦‚æœæŸäº›ä¿¡æ¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²æˆ– 0
2. å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼
3. ä¸è¦ç¼–é€ æ•°æ®ï¼Œåªä»æä¾›çš„å†…å®¹ä¸­æå–

è¿”å›æ ¼å¼ï¼š
{{
  "title": "å¸–å­æ ‡é¢˜",
  "content": "å†…å®¹æ‘˜è¦",
  "author": "ä½œè€…å",
  "likes": 1000,
  "comments": 50,
  "collects": 200,
  "top_comments": ["è¯„è®º1", "è¯„è®º2", "è¯„è®º3"]
}}
"""

        try:
            response = self.llm.generate(prompt, "ä½ æ˜¯æ•°æ®æå–ä¸“å®¶ï¼Œæ“…é•¿ä»å°çº¢ä¹¦é¡µé¢ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚")

            # è§£æ JSON
            if '{' in response and '}' in response:
                json_str = response[response.find('{'):response.rfind('}')+1]
                data = json.loads(json_str)

                topic = HotTopic(
                    title=data.get("title", title),
                    content=data.get("content", content[:300]),
                    url=url
                )
                topic.author = data.get("author", "")
                topic.likes = data.get("likes", 0)
                topic.comments = data.get("comments", 0)
                topic.collects = data.get("collects", 0)
                topic.top_comments = data.get("top_comments", [])

                return topic

        except Exception as e:
            print(f"   âš ï¸  AI æå–å¤±è´¥: {e}")

            # åˆ›å»ºåŸºç¡€ topic
            topic = HotTopic(title=title, content=content[:300], url=url)
            return topic

        return None

# ============ éœ€æ±‚åˆ†æå™¨ ============
class DemandAnalyzer:
    """éœ€æ±‚åˆ†æå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def analyze_topic(self, topic: HotTopic) -> TopicAnalysis:
        """åˆ†æå•ä¸ªçƒ­ç‚¹"""
        print(f"\nğŸ” åˆ†æçƒ­ç‚¹: {topic.title}")

        prompt = HOT_TOPIC_ANALYSIS_PROMPT.format(
            title=topic.title,
            content=topic.content,
            likes=topic.likes,
            comments=topic.comments,
            collects=topic.collects,
            top_comments="\n".join(topic.top_comments)
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            analysis = TopicAnalysis(topic)
            analysis.pain_points = data.get("pain_points", [])

            demands_data = data.get("demands", [])
            for d in demands_data:
                analysis.demands.append(d)

            analysis.commercial_value = float(data.get("priority_score", 0))
            analysis.priority = float(data.get("priority_score", 0))

            print(f"   âœ“ åˆ†æå®Œæˆï¼Œä¼˜å…ˆçº§: {analysis.priority:.1f}")

        except Exception as e:
            print(f"   âš ï¸  åˆ†æå¤±è´¥: {e}")
            # åˆ›å»ºåŸºç¡€åˆ†æ
            analysis = TopicAnalysis(topic)
            analysis.priority = 7.0

        return analysis

# ============ å†…å®¹ç”Ÿæˆå™¨ ============
class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def generate_replies(self, analysis: TopicAnalysis, num_versions: int = 3) -> ReplySet:
        """ç”Ÿæˆå›å¤"""
        print(f"\nâœï¸  ä¸º '{analysis.topic.title}' ç”Ÿæˆå›å¤...")

        pain_point = ", ".join(analysis.pain_points[:3])
        demand = analysis.demands[0]["description"] if analysis.demands else "ç”¨æˆ·éœ€è¦å¸®åŠ©"
        angle = "ç»éªŒåˆ†äº«"

        # è·å–åŸå§‹è¯„è®º
        original_comment = ""
        if analysis.topic.top_comments:
            original_comment = analysis.topic.top_comments[0]
            print(f"   ğŸ“ ç›®æ ‡è¯„è®º: {original_comment[:50]}...")

        prompt = CONTENT_GENERATION_PROMPT.format(
            title=analysis.topic.title,
            pain_point=pain_point,
            demand=demand,
            angle=angle,
            original_comment=original_comment
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            reply_set = ReplySet(
                topic_title=analysis.topic.title,
                pain_point=pain_point,
                demand=demand,
                original_comment=original_comment
            )

            for reply_data in data.get("replies", []):
                reply = GeneratedReply(
                    version=reply_data["version"],
                    angle=reply_data["angle"],
                    content=reply_data["content"],
                    relevance=reply_data["relevance_score"],
                    attractiveness=reply_data["attractiveness_score"]
                )
                reply_set.replies.append(reply)

            # é€‰æ‹©æœ€ä½³å›å¤
            if reply_set.replies:
                reply_set.best_reply = max(reply_set.replies, key=lambda r: r.overall_score)
                print(f"   âœ“ ç”Ÿæˆäº† {len(reply_set.replies)} ä¸ªç‰ˆæœ¬")
                print(f"   â˜… æœ€ä½³è¯„åˆ†: {reply_set.best_reply.overall_score:.1f}")

        except Exception as e:
            print(f"   âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
            reply_set = ReplySet(analysis.topic.title, pain_point, demand, original_comment)

        return reply_set

# ============ ä¸» Agent ============
class ManualRealAgent:
    """å°çº¢ä¹¦ Agentï¼ˆçœŸå®ç‰ˆæœ¬ - æ‰‹åŠ¨è¾“å…¥URLï¼‰"""
    def __init__(self):
        self.collector = ManualURLCollector()
        self.analyzer = DemandAnalyzer()
        self.generator = ContentGenerator()
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–"""
        if self._initialized:
            return

        # åˆ›å»ºå·¥ä½œç›®å½•
        workspace = Path("workspace/xiaohongshu")
        workspace.mkdir(parents=True, exist_ok=True)
        (workspace / "hot_topics").mkdir(parents=True, exist_ok=True)
        (workspace / "analysis").mkdir(parents=True, exist_ok=True)
        (workspace / "generated_content").mkdir(parents=True, exist_ok=True)
        (workspace / "logs").mkdir(parents=True, exist_ok=True)

        self._initialized = True
        print("âœ… Agent åˆå§‹åŒ–å®Œæˆ\n")

    async def run_single_url_workflow(self, url: str):
        """è¿è¡Œå•ä¸ªURLçš„å®Œæ•´å·¥ä½œæµ"""
        await self.initialize()

        print("\n" + "=" * 60)
        print("ğŸš€ å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥ä½œæµ - çœŸå®æ•°æ®")
        print("=" * 60)
        print("âš ï¸  åˆ†æçœŸå®çš„å°çº¢ä¹¦é“¾æ¥")
        print("=" * 60)

        # Phase 1: æ”¶é›†æ•°æ®
        print("\nğŸ“ Phase 1: ä»çœŸå®URLæ”¶é›†æ•°æ®")
        print("-" * 60)

        topic = await self.collector.collect_from_url(url)

        if not topic:
            print("\nâŒ æœªèƒ½æ”¶é›†åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            return {"success": False, "error": "æ”¶é›†æ•°æ®å¤±è´¥"}

        # Phase 2: åˆ†æéœ€æ±‚
        print("\nğŸ“ Phase 2: åˆ†æéœ€æ±‚")
        print("-" * 60)
        analysis = await self.analyzer.analyze_topic(topic)

        # Phase 3: ç”Ÿæˆå›å¤
        print("\nğŸ“ Phase 3: ç”Ÿæˆå›å¤å†…å®¹")
        print("-" * 60)
        reply_set = await self.generator.generate_replies(analysis)

        # Phase 4: è¾“å‡ºæŠ¥å‘Š
        print("\nğŸ“ Phase 4: ç”ŸæˆæŠ¥å‘Š")
        print("-" * 60)
        await self._generate_single_report(topic, analysis, reply_set)

        print("\n" + "=" * 60)
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
        print("=" * 60)

        return {
            "success": True,
            "title": topic.title,
            "url": topic.url
        }

    async def _generate_single_report(self, topic, analysis, reply_set):
        """ç”Ÿæˆå•ä¸ªè¯é¢˜çš„æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜ä¸º Markdown
        report_path = Path(f"workspace/xiaohongshu/generated_content/report_{timestamp}.md")

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ğŸ”¥ å°çº¢ä¹¦çƒ­ç‚¹åˆ†ææŠ¥å‘Šï¼ˆçœŸå®æ•°æ®ï¼‰\n\n")
            f.write(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"> æ•°æ®æ¥æº: çœŸå®å°çº¢ä¹¦é“¾æ¥\n\n")
            f.write("---\n\n")

            # è¯é¢˜è¯¦æƒ…
            f.write(f"## ğŸ“Œ è¯é¢˜è¯¦æƒ…\n\n")
            f.write(f"### {topic.title}\n\n")

            if topic.url:
                f.write(f"> ğŸ”— åŸæ–‡é“¾æ¥: [{topic.url}]({topic.url})\n\n")
                f.write(f"> âœ… çœŸå®é“¾æ¥ï¼Œå¯ç›´æ¥è®¿é—®\n\n")

            f.write(f"**ä½œè€…**: {topic.author or 'æœªçŸ¥'}\n\n")
            f.write(f"**äº’åŠ¨æ•°æ®**: ğŸ‘ {topic.likes} | ğŸ’¬ {topic.comments} | â­ {topic.collects}\n\n")
            f.write(f"**ä¼˜å…ˆçº§è¯„åˆ†**: {analysis.priority:.1f}/10\n\n")

            f.write("---\n\n")

            # å†…å®¹æ‘˜è¦
            f.write(f"## ğŸ“„ å†…å®¹æ‘˜è¦\n\n")
            f.write(f"{topic.content}\n\n")
            f.write("---\n\n")

            # ç”¨æˆ·ç—›ç‚¹
            f.write(f"## ğŸ‘¤ ç”¨æˆ·ç—›ç‚¹\n\n")
            for point in analysis.pain_points:
                f.write(f"- {point}\n")
            f.write(f"\n---\n\n")

            # çƒ­é—¨è¯„è®º
            if topic.top_comments:
                f.write(f"## ğŸ’­ çƒ­é—¨è¯„è®º\n\n")
                for idx, comment in enumerate(topic.top_comments, 1):
                    f.write(f"{idx}. > {comment}\n\n")
                f.write("\n---\n\n")

            # ç”Ÿæˆçš„å›å¤
            f.write(f"## ğŸ’¬ æ™ºèƒ½å›å¤å†…å®¹\n\n")

            # æ˜¾ç¤ºåŸå§‹è¯„è®º
            if reply_set.original_comment:
                f.write(f"### ğŸ“ åŸå§‹è¯„è®º\n\n")
                f.write(f"> {reply_set.original_comment}\n\n")

            # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„å›å¤ç‰ˆæœ¬
            f.write(f"### âœ¨ ç”Ÿæˆçš„å›å¤\n\n")

            for reply in reply_set.replies:
                score_badge = "â­" if reply.overall_score >= 9.0 else "ğŸ‘"
                f.write(f"**{score_badge} ç‰ˆæœ¬ {reply.version}: {reply.angle}**\n")
                f.write(f">(è¯„åˆ†: {reply.overall_score:.1f}/10)\n\n")
                f.write(f"{reply.content}\n\n")
                f.write("---\n\n")

            # æœ€ä½³å›å¤
            if reply_set.best_reply:
                f.write(f"### ğŸ† æœ€ä½³å›å¤æ¨è\n\n")
                f.write(f"**è¯„åˆ†**: {reply_set.best_reply.overall_score:.1f}/10\n")
                f.write(f"**è§’åº¦**: {reply_set.best_reply.angle}\n\n")
                f.write(f"{reply_set.best_reply.content}\n\n")

        print(f"   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åŒæ—¶ä¿å­˜ JSON æ•°æ®
        json_path = Path(f"workspace/xiaohongshu/analysis/analysis_{timestamp}.json")
        data = {
            "timestamp": datetime.now().isoformat(),
            "data_source": "çœŸå®å°çº¢ä¹¦é“¾æ¥",
            "topic": {
                "title": topic.title,
                "url": topic.url,
                "author": topic.author,
                "content": topic.content,
                "likes": topic.likes,
                "comments": topic.comments,
                "collects": topic.collects,
                "top_comments": topic.top_comments
            },
            "analysis": {
                "priority": analysis.priority,
                "pain_points": analysis.pain_points,
                "demands": analysis.demands
            },
            "best_reply": {
                "content": reply_set.best_reply.content if reply_set.best_reply else "",
                "angle": reply_set.best_reply.angle if reply_set.best_reply else "",
                "score": reply_set.best_reply.overall_score if reply_set.best_reply else 0
            } if reply_set.best_reply else None
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"   âœ“ æ•°æ®å·²ä¿å­˜: {json_path}")

# ============ ä¸»å‡½æ•° ============
async def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 12 + "å°çº¢ä¹¦çœŸå®æ•°æ®åˆ†æç³»ç»Ÿ" + " " * 26 + "â•‘")
    print("â•‘" + " " * 10 + "ğŸ”— è¾“å…¥çœŸå®é“¾æ¥è¿›è¡Œåˆ†æ" + " " * 24 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()

    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print(f"   âœ… DeepSeek API: å·²è¿æ¥")
    print(f"   âœ… æ¨¡å‹: deepseek-chat")
    print(f"   âœ… æ•°æ®æº: ç”¨æˆ·æä¾›çš„çœŸå®é“¾æ¥")
    print()

    # åˆ›å»º Agent
    agent = ManualRealAgent()

    # è·å–ç”¨æˆ·è¾“å…¥
    print("è¯·è¾“å…¥å°çº¢ä¹¦é“¾æ¥ï¼ˆä¾‹å¦‚: https://www.xiaohongshu.com/explore/...ï¼‰")
    url = input("\nğŸ”— URL: ").strip()

    if not url:
        print("âŒ URLä¸èƒ½ä¸ºç©º")
        return

    if not url.startswith('http'):
        url = 'https://' + url

    try:
        # è¿è¡Œå·¥ä½œæµ
        result = await agent.run_single_url_workflow(url)

        if result.get("success"):
            print()
            print("ğŸ‰ åˆ†æå®Œæˆï¼")
            print()
            print("ğŸ“ æŸ¥çœ‹ç»“æœ:")
            print("   - æŠ¥å‘Š: workspace/xiaohongshu/generated_content/")
            print("   - æ•°æ®: workspace/xiaohongshu/analysis/")
            print()

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

    input("\næŒ‰å›è½¦é”®é€€å‡º...")
