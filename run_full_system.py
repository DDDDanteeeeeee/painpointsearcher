"""
å°çº¢ä¹¦è‡ªåŠ¨åŒ–ç³»ç»Ÿ - å®Œæ•´ç‰ˆï¼ˆå¯è¿è¡Œï¼‰

ç»•è¿‡å¤æ‚ä¾èµ–ï¼Œä½¿ç”¨æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from openai import OpenAI

# ============ é…ç½® ============
DEEPSEEK_API_KEY = "sk-b07c9af227fa49b68ff1f6e4ae36465f"
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# ============ Prompt æ¨¡æ¿ ============
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå’Œåˆ›ä½œåŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **çƒ­ç‚¹æ•æ„Ÿåº¦**ï¼šèƒ½å¿«é€Ÿè¯†åˆ«å°çº¢ä¹¦å¹³å°ä¸Šçš„çƒ­ç‚¹è¯é¢˜å’Œè¶‹åŠ¿
2. **ç”¨æˆ·æ´å¯Ÿ**ï¼šæ·±åˆ»ç†è§£å°çº¢ä¹¦ç”¨æˆ·çš„å¿ƒç†ã€éœ€æ±‚å’Œç—›ç‚¹
3. **å†…å®¹åˆ›ä½œ**ï¼šèƒ½åˆ›ä½œç¬¦åˆå°çº¢ä¹¦é£æ ¼çš„é«˜è´¨é‡å†…å®¹ï¼ˆçœŸå®ã€æœ‰ç”¨ã€æœ‰æ¸©åº¦ï¼‰

ä½ çš„å·¥ä½œåŸåˆ™ï¼š
- åªåˆ†æçœŸå®çš„æ•°æ®ï¼Œä¸ç¼–é€ ä¿¡æ¯
- ç”Ÿæˆçš„å›å¤å¿…é¡»çœŸå®ã€æœ‰ä»·å€¼ï¼Œä¸èƒ½æ˜¯åƒåœ¾å¹¿å‘Š
- ä¿æŒå°çº¢ä¹¦ç¤¾åŒºçš„çœŸå®æ°›å›´
- å°Šé‡ç”¨æˆ·ï¼Œæä¾›çœŸè¯šçš„å¸®åŠ©
"""

HOT_TOPIC_ANALYSIS_PROMPT = """è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦çƒ­ç‚¹å†…å®¹ï¼ŒæŒ–æ˜æ½œåœ¨çš„ç”¨æˆ·éœ€æ±‚å’Œå•†ä¸šä»·å€¼ï¼š

## çƒ­ç‚¹å†…å®¹ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content}
äº’åŠ¨æ•°æ®ï¼šç‚¹èµ{likes} | è¯„è®º{comments} | æ”¶è—{collects}
çƒ­é—¨è¯„è®ºï¼š{top_comments}

## åˆ†æè¦æ±‚ï¼š

1. **ç”¨æˆ·ç—›ç‚¹åˆ†æ**
   - ç”¨æˆ·åœ¨æŠ±æ€¨ä»€ä¹ˆé—®é¢˜ï¼Ÿ
   - ç”¨æˆ·è¡¨è¾¾äº†ä»€ä¹ˆä¸æ»¡ï¼Ÿ
   - ç”¨æˆ·å¸Œæœ›å¾—åˆ°ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

2. **æ½œåœ¨éœ€æ±‚æŒ–æ˜**
   - è¿™ä¸ªçƒ­ç‚¹åæ˜ äº†ä»€ä¹ˆæœªè¢«æ»¡è¶³çš„éœ€æ±‚ï¼Ÿ
   - ç”¨æˆ·æ„¿æ„ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ä»˜å‡ºä»€ä¹ˆï¼ˆæ—¶é—´ã€é‡‘é’±ï¼‰ï¼Ÿ
   - éœ€æ±‚çš„ç´§æ€¥ç¨‹åº¦å’Œæ™®éç¨‹åº¦å¦‚ä½•ï¼Ÿ

3. **å•†ä¸šä»·å€¼è¯„ä¼°**
   - æ˜¯å¦æœ‰å˜ç°å¯èƒ½ï¼Ÿï¼ˆäº§å“æ¨èã€çŸ¥è¯†ä»˜è´¹ã€æœåŠ¡ç­‰ï¼‰
   - ç›®æ ‡ç”¨æˆ·ç¾¤ä½“çš„æ¶ˆè´¹èƒ½åŠ›å¦‚ä½•ï¼Ÿ
   - ç«äº‰ç¨‹åº¦å¦‚ä½•ï¼Ÿ

4. **å›å¤ç­–ç•¥å»ºè®®**
   - æœ€æœ‰æ•ˆçš„å›å¤è§’åº¦ï¼ˆä¸“ä¸š/äº§å“/ç»éªŒ/æƒ…æ„Ÿï¼‰
   - æ¨èçš„å›å¤å†…å®¹ç±»å‹
   - é¢„ä¼°çš„äº’åŠ¨ç‡

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- pain_points (æ•°ç»„)
- demands (æ•°ç»„ï¼ŒåŒ…å« type, description, urgency, commercial_value)
- commercial_potential (å­—ç¬¦ä¸²)
- suggested_angles (æ•°ç»„)
- priority_score (æ•°å­—)
"""

CONTENT_GENERATION_PROMPT = """ä¸ºå°çº¢ä¹¦å¸–å­ç”Ÿæˆé«˜è´¨é‡çš„å›å¤å†…å®¹ã€‚

## ç›®æ ‡å¸–å­ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
ç”¨æˆ·ç—›ç‚¹ï¼š{pain_point}
æ ¸å¿ƒéœ€æ±‚ï¼š{demand}
ç›®æ ‡è§’åº¦ï¼š{angle}

## å°çº¢ä¹¦å›å¤ç‰¹ç‚¹ï¼š
1. çœŸè¯šï¼šåƒçœŸäººä¸€æ ·åˆ†äº«ï¼Œä¸è¦åƒå¹¿å‘Š
2. æœ‰ç”¨ï¼šæä¾›å®é™…å¸®åŠ©æˆ–æœ‰ä»·å€¼çš„ä¿¡æ¯
3. æœ‰æ¸©åº¦ï¼šæƒ…æ„Ÿå…±é¸£ï¼Œè®©è¯»è€…æ„Ÿå—åˆ°å–„æ„
4. é€‚åº¦é•¿åº¦ï¼š50-200å­—ä¸ºå®œ

## ç”Ÿæˆè¦æ±‚ï¼š
è¯·ç”Ÿæˆ 3-5 ä¸ªä¸åŒç‰ˆæœ¬çš„å›å¤ï¼Œæ¯ä¸ªç‰ˆæœ¬è¦æœ‰ï¼š
- ç‹¬ç‰¹çš„åˆ‡å…¥è§’åº¦
- ä¸åŒçš„è¡¨è¾¾é£æ ¼
- é«˜åº¦ç›¸å…³çš„å†…å®¹
- å¸å¼•åŠ›çš„å¼€å¤´å’Œç»“å°¾

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å« replies æ•°ç»„ï¼Œæ¯ä¸ªå›å¤åŒ…å«ï¼š
- version (ç‰ˆæœ¬å·)
- angle (è§’åº¦)
- content (å†…å®¹)
- relevance_score (ç›¸å…³æ€§è¯„åˆ†)
- attractiveness_score (å¸å¼•åŠ›è¯„åˆ†)
"""

# ============ æ•°æ®æ¨¡å‹ ============
class HotTopic:
    def __init__(self, title: str, content: str, likes: int = 0, comments: int = 0, collects: int = 0, url: str = ""):
        self.title = title
        self.content = content
        self.likes = likes
        self.comments = comments
        self.collects = collects
        self.url = url  # çƒ­ç‚¹é“¾æ¥
        self.top_comments = []  # çƒ­é—¨è¯„è®º
        self.collected_at = datetime.now().isoformat()

        # åˆ†æå­—æ®µ
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class TopicAnalysis:
    def __init__(self, topic: HotTopic):
        self.topic = topic
        self.pain_points = []
        self.demands = []
        self.commercial_value = 0.0
        self.priority = 0.0

class GeneratedReply:
    def __init__(self, version: int, angle: str, content: str, relevance: float, attractiveness: float):
        self.version = version
        self.angle = angle
        self.content = content
        self.relevance_score = relevance
        self.attractiveness_score = attractiveness
        self.overall_score = (relevance + attractiveness) / 2

class ReplySet:
    def __init__(self, topic_title: str, pain_point: str, demand: str, original_comment: str = ""):
        self.topic_title = topic_title
        self.pain_point = pain_point
        self.demand = demand
        self.original_comment = original_comment  # åŸå§‹è¯„è®º
        self.replies = []
        self.best_reply = None
        self.created_at = datetime.now().isoformat()

# ============ LLM ç±» ============
class SimpleLLM:
    def __init__(self):
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )

    def generate(self, prompt: str, system_prompt: str = "") -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            max_tokens=2000,
            temperature=0.0
        )

        return response.choices[0].message.content

# ============ æ ¸å¿ƒæ¨¡å— ============
class HotTopicsCollector:
    """çƒ­ç‚¹æ”¶é›†å™¨ï¼ˆæ¨¡æ‹Ÿç‰ˆï¼‰"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def collect_from_explore_page(self, max_topics: int = 20) -> List[HotTopic]:
        """æ¨¡æ‹Ÿæ”¶é›†çƒ­ç‚¹"""
        print(f"ğŸ“ æ¨¡æ‹Ÿæ”¶é›†å°çº¢ä¹¦çƒ­ç‚¹ï¼ˆç›®æ ‡: {max_topics} ä¸ªï¼‰...")

        # ç¤ºä¾‹çƒ­ç‚¹æ•°æ®ï¼ˆåŒ…å«é“¾æ¥å’Œçƒ­é—¨è¯„è®ºï¼‰
        sample_topics = [
            {
                "title": "æ—©Cæ™šAçš„æ­£ç¡®æ‰“å¼€æ–¹å¼",
                "content": "æœ€è¿‘æŠ¤è‚¤åœˆå¾ˆç«çš„æ—©Cæ™šAï¼Œä½†å¾ˆå¤šäººéƒ½åœ¨åæ§½ï¼š1. æ—©ä¸Šç”¨äº†ç»´Cï¼Œæ™šä¸Šç”¨ç»´Aï¼Œç»“æœçš®è‚¤æ³›çº¢åˆºç—› 2. ä¸çŸ¥é“è¯¥é€‰å“ªä¸ªå“ç‰Œçš„ç²¾å 3. æ‹…å¿ƒæˆåˆ†å†²çªä¸æ•¢å æ¶‚ 4. æ•æ„Ÿè‚Œèƒ½ä¸èƒ½ç”¨ï¼Ÿä½œä¸ºæŠ¤è‚¤3å¹´çš„å°ç™½ï¼ŒçœŸçš„å¤ªå›°æƒ‘äº†ï¼æ±‚å¤§ç¥æŒ‡ç‚¹ï¼",
                "likes": 1234,
                "comments": 56,
                "collects": 789,
                "url": "https://www.xiaohongshu.com/explore/123456789",
                "top_comments": [
                    "æˆ‘ä¹Ÿè¸©è¿‡é›·ï¼åˆšå¼€å§‹ç›´æ¥ç”¨é«˜æµ“åº¦Aé†‡ï¼Œè„¸è‚¿äº†ä¸‰å¤©ğŸ˜­",
                    "æ•æ„Ÿè‚Œè·¯è¿‡ï¼Œå»ºè®®å…ˆä»0.05%å¼€å§‹ï¼Œå»ºç«‹è€å—å¾ˆé‡è¦",
                    "æœ‰æ²¡æœ‰å…·ä½“çš„äº§å“æ¨èï¼Ÿå­¦ç”Ÿå…šé¢„ç®—æœ‰é™"
                ]
            },
            {
                "title": "æç®€ç”Ÿæ´»ï¼šæ–­èˆç¦»30å¤©ï¼Œæˆ‘æ”¶è·äº†ä»€ä¹ˆ",
                "content": "åšæŒæ–­èˆç¦»30å¤©ï¼Œæ‰”æ‰äº†å®¶é‡Œä¸€åŠçš„ä¸œè¥¿ã€‚å¼€å§‹æ—¶å¾ˆéš¾ï¼Œä½†æ…¢æ…¢å‘ç°ä¸œè¥¿å°‘äº†ï¼Œå¿ƒåè€Œæ›´æ»¡äº†ã€‚ä¸å†ä¸ºä¹°ä¹°ä¹°ç„¦è™‘ï¼Œåè€Œæ›´çæƒœæ‰‹è¾¹çš„æ¯ä¸€æ ·ç‰©å“ã€‚æœ‰å§å¦¹æƒ³ä¸€èµ·å°è¯•æç®€ç”Ÿæ´»å—ï¼Ÿ",
                "likes": 2341,
                "comments": 123,
                "collects": 1567,
                "url": "https://www.xiaohongshu.com/explore/234567890",
                "top_comments": [
                    "å¤ªä½©æœäº†ï¼æˆ‘è¿æ‰”ä¸€æ”¯è¿‡æœŸå£çº¢éƒ½è¦çŠ¹è±«åŠå°æ—¶",
                    "æ–­èˆç¦»ä¸æ˜¯æ‰”ä¸œè¥¿ï¼Œæ˜¯å­¦ä¼šæ§åˆ¶æ¬²æœ›å§",
                    "æ±‚åˆ†äº«æ–­èˆç¦»æ¸…å•ï¼ä¸çŸ¥é“ä»ä½•å…¥æ‰‹"
                ]
            },
            {
                "title": "æ‰“å·¥äººå¿…çœ‹ï¼3æ¬¾æé«˜æ•ˆç‡çš„ç¥å™¨APP",
                "content": "æ¯å¤©åŠ ç­åˆ°æ·±å¤œï¼Œæ•ˆç‡ä½å¾—è®©äººæ€€ç–‘äººç”Ÿã€‚ç›´åˆ°æˆ‘å‘ç°äº†è¿™3æ¬¾APPï¼š1. ç•ªèŒ„é’Ÿ-ä¸“æ³¨å·¥ä½œ25åˆ†é’Ÿ 2. æ»´ç­”æ¸…å•-ä»»åŠ¡ç®¡ç†ç¥å™¨ 3. Forest-ç§æ ‘æˆ’æ‰‹æœºã€‚ç”¨äº†1ä¸ªæœˆï¼Œå·¥ä½œæ•ˆç‡æå‡äº†50%ï¼Œç»ˆäºä¸ç”¨åŠ ç­äº†ï¼å¼ºçƒˆæ¨èç»™æ‰€æœ‰æ‰“å·¥äººï¼",
                "likes": 3456,
                "comments": 234,
                "collects": 2345,
                "url": "https://www.xiaohongshu.com/explore/345678901",
                "top_comments": [
                    "Forestäº²æµ‹æœ‰æ•ˆï¼å°±æ˜¯æœ‰æ—¶å€™ä¼šæ‰‹æ»‘ç©æ‰‹æœºğŸ˜‚",
                    "ç•ªèŒ„é’Ÿç”¨è¿‡ï¼Œä½†æ˜¯å¼€ä¼šçš„æ—¶å€™ä¸å¥½ç”¨å•Š",
                    "æœ‰æ²¡æœ‰æ¨èMacç«¯çš„æ•ˆç‡å·¥å…·ï¼Ÿ"
                ]
            },
            {
                "title": "æ–°æ‰‹å¦ˆå¦ˆå¿…çœ‹ï¼šå®å®ç¡çœ è®­ç»ƒå…¨æ”»ç•¥",
                "content": "ç”Ÿå®Œå®å®åï¼Œæœ€ç—›è‹¦çš„å°±æ˜¯ç¡çœ é—®é¢˜ï¼å®å®å¤œé‡Œé¢‘ç¹é†’æ¥ï¼Œå¯¼è‡´æˆ‘ä¸¥é‡ç¡çœ ä¸è¶³ã€‚å°è¯•äº†å„ç§æ–¹æ³•åï¼Œç»ˆäºæ€»ç»“å‡ºä¸€å¥—æœ‰æ•ˆçš„ç¡çœ è®­ç»ƒæ–¹æ³•ã€‚ç°åœ¨å®å®èƒ½ç¡æ•´è§‰ï¼Œæˆ‘ä¹Ÿèƒ½å¥½å¥½ä¼‘æ¯äº†ï¼æ•´ç†äº†è¯¦ç»†æ”»ç•¥åˆ†äº«ç»™å„ä½æ–°æ‰‹å¦ˆå¦ˆã€‚",
                "likes": 4567,
                "comments": 345,
                "collects": 3456,
                "url": "https://www.xiaohongshu.com/explore/456789012",
                "top_comments": [
                    "å®å¦ˆå¤ªéœ€è¦äº†ï¼æ¯å¤©å‡Œæ™¨3ç‚¹å°±è¦é†’ï¼ŒçœŸçš„å¿«å´©æºƒäº†",
                    "è¯·é—®å‡ ä¸ªæœˆå¼€å§‹è®­ç»ƒæ¯”è¾ƒåˆé€‚ï¼Ÿ",
                    "æˆ‘å®¶å®å®å…­ä¸ªæœˆäº†ï¼Œè¿˜èƒ½è®­ç»ƒå—ï¼Ÿ"
                ]
            },
            {
                "title": "å‰¯ä¸šæœˆå…¥è¿‡ä¸‡ï¼Ÿæˆ‘çš„çœŸå®ç»å†åˆ†äº«",
                "content": "90åå¥³ç”Ÿï¼Œä¸»ä¸šæœˆè–ª5Kï¼Œå‰¯ä¸šåšè‡ªåª’ä½“2å¹´ï¼Œç°åœ¨æœˆå…¥ç¨³å®šè¿‡ä¸‡ã€‚ä¸æ˜¯å–è¯¾ç¨‹ï¼Œä¸æ˜¯å‰²éŸ­èœï¼Œçº¯åˆ†äº«æˆ‘çš„çœŸå®ç»å†ã€‚ä»0å¼€å§‹åšè´¦å·ï¼Œè¸©è¿‡çš„å‘ï¼Œæ€»ç»“çš„ç»éªŒã€‚æƒ³åšå‰¯ä¸šçš„å§å¦¹å¯ä»¥å‚è€ƒä¸€ä¸‹ï¼Œä½†ä¸è¦ç›²ç›®è·Ÿé£ï¼",
                "likes": 5678,
                "comments": 456,
                "collects": 4567,
                "url": "https://www.xiaohongshu.com/explore/567890123",
                "top_comments": [
                    "æ±‚è¯¦ç»†åˆ†äº«ï¼æƒ³åšå‰¯ä¸šä½†ä¸çŸ¥é“åšä»€ä¹ˆ",
                    "çœŸçš„æ˜¯ä¸å‰²éŸ­èœå—ï¼ŸæœŸå¾…åç»­",
                    "ä»€ä¹ˆèµ›é“æ¯”è¾ƒå¥½å…¥é—¨ï¼Ÿ"
                ]
            }
        ]

        topics = []
        for i, data in enumerate(sample_topics[:max_topics]):
            topic = HotTopic(
                title=data["title"],
                content=data["content"],
                likes=data["likes"],
                comments=data["comments"],
                collects=data["collects"],
                url=data.get("url", ""),
            )
            topic.top_comments = data.get("top_comments", [])
            topics.append(topic)
            print(f"   âœ“ æ”¶é›†çƒ­ç‚¹ {i+1}: {topic.title}")

        await asyncio.sleep(1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        print(f"\nâœ… æˆåŠŸæ”¶é›† {len(topics)} ä¸ªçƒ­ç‚¹")
        return topics

class DemandAnalyzer:
    """éœ€æ±‚åˆ†æå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def analyze_topic(self, topic: HotTopic) -> TopicAnalysis:
        """åˆ†æå•ä¸ªçƒ­ç‚¹"""
        print(f"\nğŸ” åˆ†æçƒ­ç‚¹: {topic.title}")

        prompt = HOT_TOPIC_ANALYSIS_PROMPT.format(
            title=topic.title,
            content=topic.content,
            likes=topic.likes,
            comments=topic.comments,
            collects=topic.collects,
            top_comments="\n".join(topic.top_comments)
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            analysis = TopicAnalysis(topic)
            analysis.pain_points = data.get("pain_points", [])

            demands_data = data.get("demands", [])
            for d in demands_data:
                analysis.demands.append(d)

            analysis.commercial_value = float(data.get("priority_score", 0))
            analysis.priority = float(data.get("priority_score", 0))

            print(f"   âœ“ åˆ†æå®Œæˆï¼Œä¼˜å…ˆçº§: {analysis.priority:.1f}")

        except Exception as e:
            print(f"   âš ï¸  åˆ†æå¤±è´¥: {e}")
            # åˆ›å»ºåŸºç¡€åˆ†æ
            analysis = TopicAnalysis(topic)
            analysis.priority = 7.0

        return analysis

    async def analyze_batch(self, topics: List[HotTopic]) -> List[TopicAnalysis]:
        """æ‰¹é‡åˆ†æ"""
        analyses = []
        for topic in topics:
            analysis = await self.analyze_topic(topic)
            analyses.append(analysis)
            await asyncio.sleep(1)  # é¿å… API é™æµ

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        analyses.sort(key=lambda x: x.priority, reverse=True)
        return analyses

class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨"""
    def __init__(self):
        self.llm = SimpleLLM()

    async def generate_replies(self, analysis: TopicAnalysis, num_versions: int = 3) -> ReplySet:
        """ç”Ÿæˆå›å¤"""
        print(f"\nâœï¸  ä¸º '{analysis.topic.title}' ç”Ÿæˆå›å¤...")

        pain_point = ", ".join(analysis.pain_points[:3])
        demand = analysis.demands[0]["description"] if analysis.demands else "ç”¨æˆ·éœ€è¦å¸®åŠ©"
        angle = "ç»éªŒåˆ†äº«"

        # è·å–åŸå§‹è¯„è®ºï¼ˆé€‰æ‹©ç¬¬ä¸€æ¡çƒ­é—¨è¯„è®ºä½œä¸ºç›®æ ‡ï¼‰
        original_comment = ""
        if analysis.topic.top_comments:
            original_comment = analysis.topic.top_comments[0]
            print(f"   ğŸ“ ç›®æ ‡è¯„è®º: {original_comment[:50]}...")

        prompt = CONTENT_GENERATION_PROMPT.format(
            title=analysis.topic.title,
            pain_point=pain_point,
            demand=demand,
            angle=angle
        )

        try:
            response = self.llm.generate(prompt, SYSTEM_PROMPT)

            # è§£æ JSON å“åº”
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            reply_set = ReplySet(
                topic_title=analysis.topic.title,
                pain_point=pain_point,
                demand=demand,
                original_comment=original_comment
            )

            for reply_data in data.get("replies", []):
                reply = GeneratedReply(
                    version=reply_data["version"],
                    angle=reply_data["angle"],
                    content=reply_data["content"],
                    relevance=reply_data["relevance_score"],
                    attractiveness=reply_data["attractiveness_score"]
                )
                reply_set.replies.append(reply)

            # é€‰æ‹©æœ€ä½³å›å¤
            if reply_set.replies:
                reply_set.best_reply = max(reply_set.replies, key=lambda r: r.overall_score)
                print(f"   âœ“ ç”Ÿæˆäº† {len(reply_set.replies)} ä¸ªç‰ˆæœ¬")
                print(f"   â˜… æœ€ä½³è¯„åˆ†: {reply_set.best_reply.overall_score:.1f}")

        except Exception as e:
            print(f"   âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
            reply_set = ReplySet(analysis.topic.title, pain_point, demand)

        return reply_set

# ============ ä¸» Agent ============
class XiaohongshuAgent:
    """å°çº¢ä¹¦ Agentï¼ˆå®Œæ•´ç‰ˆï¼‰"""
    def __init__(self):
        self.collector = HotTopicsCollector()
        self.analyzer = DemandAnalyzer()
        self.generator = ContentGenerator()
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–"""
        if self._initialized:
            return

        # åˆ›å»ºå·¥ä½œç›®å½•
        workspace = Path("workspace/xiaohongshu")
        workspace.mkdir(parents=True, exist_ok=True)
        (workspace / "hot_topics").mkdir(parents=True, exist_ok=True)
        (workspace / "analysis").mkdir(parents=True, exist_ok=True)
        (workspace / "generated_content").mkdir(parents=True, exist_ok=True)
        (workspace / "logs").mkdir(parents=True, exist_ok=True)

        self._initialized = True
        print("âœ… Agent åˆå§‹åŒ–å®Œæˆ\n")

    async def run_full_workflow(self, max_topics: int = 20, max_replies: int = 5):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        await self.initialize()

        print("\n" + "=" * 60)
        print("ğŸš€ å°çº¢ä¹¦è‡ªåŠ¨åŒ–å·¥ä½œæµ - å¼€å§‹æ‰§è¡Œ")
        print("=" * 60)

        # Phase 1: æ”¶é›†çƒ­ç‚¹
        print("\nğŸ“ Phase 1: æ”¶é›†çƒ­ç‚¹")
        print("-" * 60)
        topics = await self.collector.collect_from_explore_page(max_topics)

        # Phase 2: åˆ†æéœ€æ±‚
        print("\nğŸ“ Phase 2: åˆ†æéœ€æ±‚")
        print("-" * 60)
        analyses = await self.analyzer.analyze_batch(topics)

        print(f"\nğŸ† TOP {min(len(analyses), max_replies)} é«˜ä»·å€¼è¯é¢˜:")
        for i, analysis in enumerate(analyses[:max_replies], 1):
            print(f"   {i}. {analysis.topic.title} (ä¼˜å…ˆçº§: {analysis.priority:.1f})")

        # Phase 3: ç”Ÿæˆå›å¤
        print("\nğŸ“ Phase 3: ç”Ÿæˆå›å¤å†…å®¹")
        print("-" * 60)
        reply_sets = []

        for i, analysis in enumerate(analyses[:max_replies], 1):
            reply_set = await self.generator.generate_replies(analysis)
            if reply_set.replies:
                reply_sets.append(reply_set)

            if i < len(analyses[:max_replies]) - 1:
                await asyncio.sleep(2)  # æ¨¡æ‹Ÿå»¶è¿Ÿ

        # Phase 4: è¾“å‡ºæŠ¥å‘Š
        print("\nğŸ“ Phase 4: ç”ŸæˆæŠ¥å‘Š")
        print("-" * 60)
        await self._generate_report(topics, analyses, reply_sets)

        print("\n" + "=" * 60)
        print("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼")
        print("=" * 60)

        return {
            "success": True,
            "topics_collected": len(topics),
            "topics_analyzed": len(analyses),
            "replies_generated": len(reply_sets)
        }

    async def _generate_report(self, topics, analyses, reply_sets):
        """ç”ŸæˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜ä¸º Markdown
        report_path = Path(f"workspace/xiaohongshu/generated_content/report_{timestamp}.md")

        with open(report_path, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’Œæ¦‚è§ˆ
            f.write(f"# ğŸ”¥ å°çº¢ä¹¦çƒ­ç‚¹åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")

            # ç»Ÿè®¡æ¦‚è§ˆ
            f.write(f"## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ\n\n")
            f.write(f"| æŒ‡æ ‡ | æ•°é‡ |\n")
            f.write(f"|------|------|\n")
            f.write(f"| æ”¶é›†çƒ­ç‚¹ | {len(topics)} ä¸ª |\n")
            f.write(f"| åˆ†æè¯é¢˜ | {len(analyses)} ä¸ª |\n")
            f.write(f"| ç”Ÿæˆå›å¤ | {len(reply_sets)} ç»„ |\n\n")
            f.write("---\n\n")

            # TOP é«˜ä»·å€¼è¯é¢˜
            f.write(f"## ğŸ† TOP é«˜ä»·å€¼è¯é¢˜\n\n")
            for i, analysis in enumerate(analyses[:len(reply_sets)], 1):
                topic = analysis.topic

                # è¯é¢˜æ ‡é¢˜å’Œé“¾æ¥
                f.write(f"### {i}. {topic.title}\n\n")

                if topic.url:
                    f.write(f"> ğŸ”— åŸæ–‡é“¾æ¥: [{topic.url}]({topic.url})\n\n")

                # æ•°æ®å¡ç‰‡
                f.write(f"**äº’åŠ¨æ•°æ®**: ğŸ‘ {topic.likes} | ğŸ’¬ {topic.comments} | â­ {topic.collects}\n\n")
                f.write(f"**ä¼˜å…ˆçº§è¯„åˆ†**: {analysis.priority:.1f}/10\n\n")

                # ç”¨æˆ·ç—›ç‚¹
                f.write(f"#### ğŸ‘¤ ç”¨æˆ·ç—›ç‚¹\n\n")
                for point in analysis.pain_points:
                    f.write(f"- {point}\n")
                f.write(f"\n")

                # çƒ­é—¨è¯„è®º
                if topic.top_comments:
                    f.write(f"#### ğŸ’­ çƒ­é—¨è¯„è®º\n\n")
                    for idx, comment in enumerate(topic.top_comments[:3], 1):
                        f.write(f"{idx}. > {comment}\n\n")
                    f.write("\n")

                f.write("---\n\n")

            # ç”Ÿæˆçš„å›å¤
            f.write(f"## ğŸ’¬ æ™ºèƒ½å›å¤å†…å®¹\n\n")

            for reply_set in reply_sets[:3]:
                f.write(f"### {reply_set.topic_title}\n\n")

                # æ˜¾ç¤ºåŸå§‹è¯„è®ºï¼ˆå¦‚æœæœ‰ï¼‰
                if reply_set.original_comment:
                    f.write(f"#### ğŸ“ åŸå§‹è¯„è®º\n\n")
                    f.write(f"> {reply_set.original_comment}\n\n")

                # æ˜¾ç¤ºæ‰€æœ‰ç”Ÿæˆçš„å›å¤ç‰ˆæœ¬
                f.write(f"#### âœ¨ ç”Ÿæˆçš„å›å¤\n\n")

                for reply in reply_set.replies:
                    score_badge = "â­" if reply.overall_score >= 9.0 else "ğŸ‘"
                    f.write(f"**{score_badge} ç‰ˆæœ¬ {reply.version}: {reply.angle}**\n")
                    f.write(f">(è¯„åˆ†: {reply.overall_score:.1f}/10)\n\n")
                    f.write(f"{reply.content}\n\n")
                    f.write("---\n\n")

                # æœ€ä½³å›å¤
                if reply_set.best_reply:
                    f.write(f"#### ğŸ† æœ€ä½³å›å¤æ¨è\n\n")
                    f.write(f"**è¯„åˆ†**: {reply_set.best_reply.overall_score:.1f}/10\n")
                    f.write(f"**è§’åº¦**: {reply_set.best_reply.angle}\n\n")
                    f.write(f"{reply_set.best_reply.content}\n\n")
                    f.write("---\n\n")

            # æ€»ç»“
            f.write(f"## ğŸ“ åˆ†ææ€»ç»“\n\n")
            f.write(f"æœ¬æ¬¡å…±åˆ†æäº† {len(topics)} ä¸ªå°çº¢ä¹¦çƒ­ç‚¹è¯é¢˜ï¼Œè¯†åˆ«å‡º {len(reply_sets)} ä¸ªé«˜ä»·å€¼å†…å®¹æœºä¼šã€‚\n\n")
            f.write(f"æ‰€æœ‰ç”Ÿæˆçš„å›å¤å†…å®¹å‡åŸºäºç”¨æˆ·ç—›ç‚¹å’Œéœ€æ±‚åˆ†æï¼Œç¡®ä¿å†…å®¹çš„ç›¸å…³æ€§å’Œå¸å¼•åŠ›ã€‚\n\n")

        print(f"   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åŒæ—¶ä¿å­˜ JSON æ•°æ®ï¼ˆåŒ…å«æ›´å®Œæ•´çš„ä¿¡æ¯ï¼‰
        json_path = Path(f"workspace/xiaohongshu/analysis/analysis_{timestamp}.json")
        data = {
            "timestamp": datetime.now().isoformat(),
            "topics": [
                {
                    "title": t.title,
                    "url": t.url,
                    "likes": t.likes,
                    "comments": t.comments,
                    "collects": t.collects,
                    "top_comments": t.top_comments
                } for t in topics
            ],
            "analyses": [
                {
                    "title": a.topic.title,
                    "priority": a.priority,
                    "pain_points": a.pain_points,
                    "demands": a.demands
                } for a in analyses
            ],
            "reply_sets": [
                {
                    "topic_title": rs.topic_title,
                    "original_comment": rs.original_comment,
                    "best_reply": {
                        "content": rs.best_reply.content if rs.best_reply else "",
                        "angle": rs.best_reply.angle if rs.best_reply else "",
                        "score": rs.best_reply.overall_score if rs.best_reply else 0
                    } if rs.best_reply else None
                } for rs in reply_sets
            ]
        }

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"   âœ“ æ•°æ®å·²ä¿å­˜: {json_path}")

# ============ ä¸»å‡½æ•° ============
async def main():
    """ä¸»å‡½æ•°"""
    print()
    print("â•”" + "=" * 58 + "â•—")
    print("â•‘" + " " * 15 + "å°çº¢ä¹¦è‡ªåŠ¨åŒ–ç³»ç»Ÿ" + " " * 27 + "â•‘")
    print("â•‘" + " " * 10 + "ğŸ”¥ å®Œæ•´ç‰ˆå·¥ä½œæµ - çƒ­ç‚¹æŒ–æ˜ & æ™ºèƒ½å›å¤" + " " * 16 + "â•‘")
    print("â•š" + "=" * 58 + "â•")
    print()

    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print(f"   âœ… DeepSeek API: å·²è¿æ¥")
    print(f"   âœ… æ¨¡å‹: deepseek-chat")
    print(f"   âœ… å·¥ä½œç©ºé—´: workspace/xiaohongshu/")
    print()

    # åˆ›å»º Agent
    agent = XiaohongshuAgent()

    try:
        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        result = await agent.run_full_workflow(
            max_topics=5,  # æ”¶é›†5ä¸ªçƒ­ç‚¹
            max_replies=3   # ä¸ºå‰3ä¸ªç”Ÿæˆå›å¤
        )

        print()
        print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        print()
        print("ğŸ“ æŸ¥çœ‹ç»“æœ:")
        print("   - æŠ¥å‘Š: workspace/xiaohongshu/generated_content/")
        print("   - æ•°æ®: workspace/xiaohongshu/analysis/")
        print()

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

    input("\næŒ‰å›è½¦é”®é€€å‡º...")
